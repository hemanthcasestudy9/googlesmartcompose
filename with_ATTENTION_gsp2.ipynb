{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "with_ATTENTION_gsp2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "# Google Smart Composer\n",
        "\n",
        "<img src = \"https://drive.google.com/open?id=1xw4Jh-0rRhA1e5X1lH8bLjwmyeSCcE62\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "outputId": "b6f66af6-58e1-4a59-e820-13741bf2e24b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from nltk.translate.bleu_score import sentence_bleu as score\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPA1-jK4pwvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Data: \n",
        "Data is taken from personal emails and constructed according to problem specific.\n",
        "\n",
        "### Structure of data:\n",
        "From all the emails 'to', 'subject', 'previous email'(if any) and 'content' parts are taken. As we are going to predict next comming word in the content  part, content part is breaked into following way. We are restriction our self to predict only atmost 5 next comming words so each email content will be breaked to many sentances.\n",
        "<br><b>Ex:</b> Content: This is introduction to my project<br>\n",
        " <table>\n",
        "    <tr>\n",
        "      <th>Sentance</th>\n",
        "      <th>Output</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This </td>\n",
        "      <td> is</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This </td>\n",
        "      <td> is introduction </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This </td>\n",
        "      <td> is introduction to</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This </td>\n",
        "      <td> is introduction to my</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This </td>\n",
        "      <td> is introduction to my project</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This is </td>\n",
        "      <td> introduction</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This is</td>\n",
        "      <td> introduction to </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This is </td>\n",
        "      <td> introduction to my</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This is</td>\n",
        "      <td> introduction to my</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This is</td>\n",
        "      <td> introduction to my project</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This is introduction</td>\n",
        "      <td> to</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This is introduction</td>\n",
        "      <td>  to my </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td> This is introduction </td>\n",
        "      <td> to my project</td>\n",
        "    </tr>\n",
        "    \n",
        "   \n",
        " </table>\n",
        "\n",
        "Now to the sentance part 'to','subject', 'previous email' parts are joined with their corresponding separaters.<br>\n",
        "<b>Ex:</b> < to > email@ email.com< sub > introduction < prv > nan < cont > hello email this is only an intro \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "outputId": "511e695e-3ca4-42cd-8e83-d76bdc06e3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxZn7jFnp0fC",
        "colab_type": "code",
        "outputId": "ad8a0f2a-db56-43be-85fd-cb32d3acb68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.download('gdrive/My Drive/google/final_data_my_emails.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 57514, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpCUcHZiavXq",
        "colab_type": "code",
        "outputId": "1b239c10-52f2-4ec8-9db3-4342b06225a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3MrnQGDWEE",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSGthirAeNPh",
        "colab_type": "code",
        "outputId": "84fdbb29-54d1-4ebd-83dd-07f46d3ee3cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "data = pd.read_csv('gdrive/My Drive/google/final_data_my_emails.csv')\n",
        "print('Number of rows in data',data.shape[0])\n",
        "print('Number of columns in data',data.shape[1])\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in data 181255\n",
            "Number of columns in data 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;to&gt; yernagulahemanth &lt;sub&gt;  how are you commi...</td>\n",
              "      <td>yernagulahemanth,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;to&gt; yernagulahemanth &lt;sub&gt;  how are you commi...</td>\n",
              "      <td>yernagulahemanth, i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;to&gt; yernagulahemanth &lt;sub&gt;  how are you commi...</td>\n",
              "      <td>yernagulahemanth, i will</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;to&gt; yernagulahemanth &lt;sub&gt;  how are you commi...</td>\n",
              "      <td>yernagulahemanth, i will be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;to&gt; yernagulahemanth &lt;sub&gt;  how are you commi...</td>\n",
              "      <td>yernagulahemanth, i will be coming</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   x                                    y\n",
              "0  <to> yernagulahemanth <sub>  how are you commi...                    yernagulahemanth,\n",
              "1  <to> yernagulahemanth <sub>  how are you commi...                  yernagulahemanth, i\n",
              "2  <to> yernagulahemanth <sub>  how are you commi...             yernagulahemanth, i will\n",
              "3  <to> yernagulahemanth <sub>  how are you commi...          yernagulahemanth, i will be\n",
              "4  <to> yernagulahemanth <sub>  how are you commi...   yernagulahemanth, i will be coming"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTbSbBz55QtF",
        "outputId": "e6a36b0a-eb43-4989-e191-0e234aa4cda9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "data.y = data.y.apply(lambda x:str(x)) # y part was having now int values so they are converted to string values\n",
        "\n",
        "# Along with defalt tags x and y are added with <start> and <end> tags at starting and ending \n",
        "data.x = data.x.apply(lambda x:'<start> ' + str(x) + ' <end>')\n",
        "data.y = data.y.apply(lambda x:'<start> ' + str(x) + ' <end>')\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;start&gt; &lt;start&gt; &lt;to&gt; yernagulahemanth &lt;sub&gt;  h...</td>\n",
              "      <td>&lt;start&gt; &lt;start&gt;  yernagulahemanth, &lt;end&gt; &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;start&gt; &lt;start&gt; &lt;to&gt; yernagulahemanth &lt;sub&gt;  h...</td>\n",
              "      <td>&lt;start&gt; &lt;start&gt;  yernagulahemanth, i &lt;end&gt; &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;start&gt; &lt;start&gt; &lt;to&gt; yernagulahemanth &lt;sub&gt;  h...</td>\n",
              "      <td>&lt;start&gt; &lt;start&gt;  yernagulahemanth, i will &lt;end...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;start&gt; &lt;start&gt; &lt;to&gt; yernagulahemanth &lt;sub&gt;  h...</td>\n",
              "      <td>&lt;start&gt; &lt;start&gt;  yernagulahemanth, i will be &lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;start&gt; &lt;start&gt; &lt;to&gt; yernagulahemanth &lt;sub&gt;  h...</td>\n",
              "      <td>&lt;start&gt; &lt;start&gt;  yernagulahemanth, i will be c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   x                                                  y\n",
              "0  <start> <start> <to> yernagulahemanth <sub>  h...     <start> <start>  yernagulahemanth, <end> <end>\n",
              "1  <start> <start> <to> yernagulahemanth <sub>  h...   <start> <start>  yernagulahemanth, i <end> <end>\n",
              "2  <start> <start> <to> yernagulahemanth <sub>  h...  <start> <start>  yernagulahemanth, i will <end...\n",
              "3  <start> <start> <to> yernagulahemanth <sub>  h...  <start> <start>  yernagulahemanth, i will be <...\n",
              "4  <start> <start> <to> yernagulahemanth <sub>  h...  <start> <start>  yernagulahemanth, i will be c..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqsxUmfBYfpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing x and y\n",
        "# Tokenizing: Collecting all tokens(words) for x and y and storing them  with index number\n",
        "\n",
        "X_  = list(data.x.values)\n",
        "Y_ = list(data.y.values)\n",
        "\n",
        "x_tokenizer   = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "x_tokenizer.fit_on_texts(X_)\n",
        "x_tensor         = x_tokenizer.texts_to_sequences(X_)\n",
        "\n",
        "# padding\n",
        "x_tensor         = tf.keras.preprocessing.sequence.pad_sequences(x_tensor,padding='post')\n",
        "\n",
        "\n",
        "y_tokenizer  = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "y_tokenizer.fit_on_texts(Y_)\n",
        "y_tensor        = y_tokenizer.texts_to_sequences(Y_)\n",
        "y_tensor        = tf.keras.preprocessing.sequence.pad_sequences(y_tensor,padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm4BF5aKvK1Z",
        "colab_type": "code",
        "outputId": "b2042d13-6fa4-4d7d-a4da-660d6c7e5fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print('Total number of words in x:',len(x_tokenizer.index_word))\n",
        "print('Total number of words in y:',len(y_tokenizer.index_word))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in x: 6082\n",
            "Total number of words in y: 6006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "outputId": "99578782-4675-48a8-fce9-e3a362c25b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Making train and validation data with 80-20 ratio \n",
        "x_train, x_val, y_train, y_val = train_test_split(x_tensor, y_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "y_token_max_len, x_token_max_len = max([len(i) for i in y_tensor]), max([len(i) for i in x_tensor])\n",
        "\n",
        "print('Number of data points in xtrain:',len(x_train))\n",
        "print('Number of data points in ytrain:',len(y_train))\n",
        "print('Number of data points in xval  :',len(x_val))\n",
        "print('Number of data points in yval  :',len(y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points in xtrain: 145004\n",
            "Number of data points in ytrain: 145004\n",
            "Number of data points in xval  : 36251\n",
            "Number of data points in yval  : 36251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK23lXf95lNj",
        "colab_type": "code",
        "outputId": "1f0a81fc-ae95-4c4b-c8be-e60498293512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_token_max_len,x_token_max_len"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 803)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "## Feeding this data into tensorflow data api\n",
        "we can make operation like shuffleing, getting batch wise so as to make future simple\n",
        "\n",
        "`Ref: https://www.tensorflow.org/guide/data'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "Batch_Size = 100\n",
        "steps_per_epoch = len(x_train)//Batch_Size\n",
        "Emd_dim = 200\n",
        "units = 500\n",
        "x_vocab_size = len(x_tokenizer.word_index)+1\n",
        "y_vocab_size = len(y_tokenizer.word_index)+1\n",
        "\n",
        "# Shuffling the data set \n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train))\n",
        "# Getting batch wise and droping last batch if it is not having less number of points(diss similar batch)\n",
        "dataset = dataset.batch(Batch_Size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZWvTtIgdFoc",
        "colab_type": "code",
        "outputId": "d7e1f2e0-75dd-4259-f136-31357f4f38f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([100, 803]), TensorShape([100, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## Encoder Decoder With ATTENTION\n",
        "---------put that big image here ------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  '''\n",
        "    vocab_size   - Vocabulary size\n",
        "    Emd_dim      - Number of emding dimensions\n",
        "    batch_size   - Batch Size\n",
        "    \n",
        "  '''\n",
        "  def __init__(self, vocab_size, Emd_dim, enc_units, batch_size,eps = 0.001,drp_rt = 0.2):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, Emd_dim)\n",
        "    # self.embedding = MyLayer(vocab_size,Emd_dim,enc_units)\n",
        "    self.encoder = tf.compat.v1.keras.layers.CuDNNLSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.batch_norm = tf.keras.layers.LayerNormalization(epsilon=eps)\n",
        "    self.dropout = tf.keras.layers.Dropout(rate=drp_rt)\n",
        "  \n",
        "  def call(self, input_, hidden,train):\n",
        "\n",
        "    '''\n",
        "      Given input and hidden states for encoder returns the updated hidden states and output\n",
        "    '''\n",
        "    # with tf.device('/gpu:0'):\n",
        "    input_ = self.embedding(input_)\n",
        "    # print(input_.shape)\n",
        "    # print(x.shape,hidden[0].shape,hidden[1].shape)\n",
        "    output, state_h,state_c = self.encoder(input_, initial_state = hidden)\n",
        "    # output = self.dropout(output,training = train)\n",
        "    # output = self.batch_norm(output)\n",
        "    # state = tf.concat([state_h,state_c],axis=1)\n",
        "    state = [state_h,state_c]\n",
        "    return output, state\n",
        "\n",
        " \n",
        "  def initialize_hidden_state(self):\n",
        "      '''\n",
        "       Returns initial states for encoder\n",
        "      '''\n",
        "      initial_states = [tf.zeros((self.batch_size, self.enc_units)),tf.zeros((self.batch_size, self.enc_units))]\n",
        "      return initial_states\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "outputId": "37a05ade-9ba0-4dd3-f2fe-ece849d2a228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "encoder = Encoder(x_vocab_size, Emd_dim, units, 100)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden,False)\n",
        "\n",
        "\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "# print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a0fe2e52d37e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msample_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_input_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-2231415c9a4a>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_, hidden, train)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# print(input_.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# print(x.shape,hidden[0].shape,hidden[1].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m# output = self.dropout(output,training = train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# output = self.batch_norm(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constants'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m   def call(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/cudnn_recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;31m# Reverse time axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/cudnn_recurrent.py\u001b[0m in \u001b[0;36m_process_batch\u001b[0;34m(self, inputs, initial_state)\u001b[0m\n\u001b[1;32m    504\u001b[0m     }\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_cudnn_rnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnnv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnnv2\u001b[0;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0minput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m             ctx=_ctx)\n\u001b[0m\u001b[1;32m   1729\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnnv2_eager_fallback\u001b[0;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\u001b[0m\n\u001b[1;32m   1805\u001b[0m   \"is_training\", is_training)\n\u001b[1;32m   1806\u001b[0m   _result = _execute.execute(b\"CudnnRNNV2\", 5, inputs=_inputs_flat,\n\u001b[0;32m-> 1807\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   1808\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m     _execute.record_gradient(\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find valid device for node.\nNode:{{node CudnnRNNV2}}\nAll kernels registered for op CudnnRNNV2 :\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n [Op:CudnnRNNV2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    self.units = units\n",
        "\n",
        "  def call(self, hid, enc_out):\n",
        "\n",
        "    # hidden shape == (Batch_Size, hidden size)\n",
        "    # hidden_with_time_axis shape == (Batch_Size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    # hidden_with_time_axis = tf.expand_dims(hid, 1)\n",
        "    # Since Lstm layers will give us two states i.e cell state and hidden state \n",
        "    # we are passing hidden state through dense layer and cell state through dence state\n",
        "    # and adding both outputs of dence  layer\n",
        "    hidden_with_time_axis  = tf.expand_dims(self.W2(hid[0])+self.W2(hid[1]),1)\n",
        "    # print(hidden_with_time_axis.shape)\n",
        "    # Now this sum of  outputs from dense layers is added with outputs of  encoder output and \n",
        "    # along with tanh and a single unit dense layer\n",
        "    # score shape == (Batch_Size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(enc_out) + hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * enc_out\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k534zTHiDjQU",
        "colab": {}
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "sample_context, sample_atweights = attention_layer(sample_hidden, sample_output)\n",
        "print('Attention context  shape:',sample_context.shape)\n",
        "print('Attention weights shape :',sample_atweights.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, Emd_dim, dec_units, batch_size,nwtpa = 1000,eps=0.001,drp_rt=0.1):\n",
        "    ''' \n",
        "    All the variables are same as previous encoder calss except nwtpa\n",
        "    nwtpa is number of units you want to maintain at attention mechanism\n",
        "    '''\n",
        "\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, Emd_dim)\n",
        "    # self.embedding = MyLayer(vocab_size,  Emd_dim,dec_units)\n",
        "    self.decoder =  tf.compat.v1.keras.layers.CuDNNLSTM(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.batch_norm = tf.keras.layers.LayerNormalization(epsilon=eps)\n",
        "    self.dropout    = tf.keras.layers.Dropout(drp_rt)\n",
        "    # self.time_dist  = (tf.keras.layers.Dense(2),input_shape=(100,500))\n",
        "    self.fc =tf.keras.layers.Dense(vocab_size,activation='softmax')\n",
        "    self.nwtpa = nwtpa\n",
        "    self.time_dist1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(50))\n",
        "    self.time_dist2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(15))\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.nwtpa)\n",
        "\n",
        "  def call(self, input_, hidden, enc_output):\n",
        "    with tf.device('/gpu:0'):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        # passing  hidden states and encoder output to the attention layer\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        # context_vector = self.batch_norm(context_vector)\n",
        "        # context_vector = self.dropout(context_vector)\n",
        "        # applying embdding to the inputs of the dense layer\n",
        "        # input_ shape after passing through embedding == (batch_size, 1, Emd_dim)\n",
        "        input_ = self.embedding(input_)\n",
        "\n",
        "        # concating previous input and context vector\n",
        "        # input_ shape after concatenation == (batch_size, 1, Emd_dim + hidden_size)\n",
        "        input_ = tf.concat([tf.expand_dims(context_vector, 1), input_], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the Lstm\n",
        "        output, state_h,state_c = self.decoder(input_)\n",
        "        state = [state_h,state_c]\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = self.time_dist1(output)\n",
        "        output = self.time_dist2(output)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        # output = self.time_dist(output)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(y_vocab_size, Emd_dim, units, Batch_Size)\n",
        "sample_deout, sample_destate,sample_atweights_ = decoder(tf.random.uniform((Batch_Size, 1)),sample_hidden,sample_output)\n",
        "print('shape of decoder output:',sample_deout.shape)\n",
        "# print('shape of decoder state :',sample_destate.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(actual, pred):\n",
        "  with tf.device('/gpu:0'):\n",
        "      mask = tf.math.logical_not(tf.math.equal(actual, 0))\n",
        "      loss_ = loss_object(actual, pred)\n",
        "\n",
        "      mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "      loss_ *= mask\n",
        "\n",
        "      return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Creating Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "import time\n",
        "# print(str(time.localtime().tm_hour)+'_'+str(time.localtime().tm_min)+'_'+str(time.localtime().tm_sec))\n",
        "checkpoint_dir = 'gdrive/My Drive/google/checkpoints/check'\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCiOZr3Iy8X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(sentence):\n",
        "  with tf.device('/gpu:0'):\n",
        "      \n",
        "      # Tokenizing inputs\n",
        "      # inputs = [x_tokenizer.word_index[i] for i in sentence.split()]\n",
        "      # Padding inputs\n",
        "      inputs = sentence\n",
        "      inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                            maxlen=x_token_max_len,\n",
        "                                                            padding='post')\n",
        "      inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "      result = ''\n",
        "\n",
        "      # Making initial hidden states for encoder\n",
        "      hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n",
        "      \n",
        "      # inputs and hidden states are passed to encoder\n",
        "      enc_out, enc_hidden = encoder(inputs, hidden,train = False)\n",
        "      \n",
        "\n",
        "      dec_hidden = enc_hidden\n",
        "\n",
        "      # Making inputs for decoder as <start>\n",
        "      dec_input = tf.expand_dims([y_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "      \n",
        "      for t in range(y_token_max_len):\n",
        "              # inputing decoder input(<start> during initial sate), encoder hidden state and encoder output\n",
        "              predictions, dec_hidden,_ = decoder(dec_input, dec_hidden,enc_out)\n",
        "      \n",
        "              predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "              \n",
        "\n",
        "              result += y_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "              # if the predicted word is <end> returning the words predicted till now\n",
        "              if y_tokenizer.index_word[predicted_id] == '<end>':\n",
        "                return result\n",
        "\n",
        "              # current output from decoder is fed back to decoder into next timestep\n",
        "              dec_input = tf.expand_dims([predicted_id], 0)\n",
        "      \n",
        "      return result\n",
        "\n",
        "\n",
        "\n",
        "def predict_next_word(sentence):\n",
        "      result = test(sentence)\n",
        "      return result\n",
        "\n",
        "\n",
        "def bleu_score(actual,pred):\n",
        "#     print('Actual:',actual)\n",
        "#     print('Predic:',pred)\n",
        "    return score([actual],pred)\n",
        "\n",
        "\n",
        "def xgenerate_sent(data,id_):\n",
        "\n",
        "        test_with = []\n",
        "        try:\n",
        "          for i in data[id_]:\n",
        "            test_with.append(x_tokenizer.index_word[i])\n",
        "        except:\n",
        "\n",
        "          return ' '.join(test_with)\n",
        "\n",
        "        return ' '.join(test_with)\n",
        "\n",
        "def ygenerate_sent(data,id_):\n",
        "\n",
        "        test_with = []\n",
        "        \n",
        "        try:\n",
        "          for i in data[id_]:\n",
        "            test_with.append(y_tokenizer.index_word[i])\n",
        "        except:\n",
        "          \n",
        "          return ' '.join(test_with)\n",
        "        \n",
        "        return ' '.join(test_with)        \n",
        "\n",
        "\n",
        "def log_perplexity(prob):\n",
        "      \n",
        "      return np.dot(np.log(prob),prob)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def score_calc (type_,x=0,y=0,nm='0'):\n",
        "\n",
        "      total_score = []\n",
        "      if type_   == 'log_perplex':\n",
        "          \n",
        "          for i in tqdm(range(len(x)),position=0):\n",
        "              \n",
        "              total_score.append(log_perplexity(test(x[i])[1]))\n",
        "              \n",
        "          return -np.mean(total_score)\n",
        "          \n",
        "\n",
        "      elif type_ == 'bleu':\n",
        "          \n",
        "          for i in tqdm(range(len(x)),position=0):\n",
        "                  \n",
        "                  total_score.append(bleu_score(y[i],test(x[i])[0]))\n",
        "          return np.mean(total_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8HsZhYhA01D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_ = []\n",
        "y_train_ = []\n",
        "for i in tqdm(range(x_train.shape[0]),position=0):\n",
        "    x_train_.append(xgenerate_sent(x_train,i))\n",
        "    y_train_.append(ygenerate_sent(y_train,i))\n",
        "\n",
        "\n",
        "x_val_ = []\n",
        "y_val_ = []\n",
        "for i in tqdm(range(x_val.shape[0]),position=0):\n",
        "    x_val_.append(xgenerate_sent(x_val,i))\n",
        "    y_val_.append(ygenerate_sent(y_val,i))    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3zny5GCHzMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inputs = x_train_[0]\n",
        "# inputs = [x_tokenizer.word_index[i] for i in inputs.split()]\n",
        "# inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "#                                                             maxlen=x_token_max_len,\n",
        "#                                                             padding='post')\n",
        "# inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "# inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "colab": {}
      },
      "source": [
        "def train(EPOCHS = 10,verbose = 0,optimizer= tf.keras.optimizers.Adam()):\n",
        "    with tf.device('/gpu:0'):\n",
        "        initial_score = 10\n",
        "        score_500     = 0 \n",
        "        logdir = \"logs/train_data/\"\n",
        "        model_name = str(optimizer).split('object at')[0].split('.')[-1].strip()\n",
        "        train_writer = tf.summary.create_file_writer(logdir+str('model_{}'.format(model_name)))\n",
        "        with train_writer.as_default():\n",
        "          for epoch in range(EPOCHS):\n",
        "                print('\\nTraining Epoch: {}'.format(epoch+1))\n",
        "                epoch_score = []\n",
        "                start = time.time()\n",
        "\n",
        "                enc_hidden = encoder.initialize_hidden_state()\n",
        "                total_loss = 0\n",
        "\n",
        "                for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "                        # batch_loss = train_step(inp, targ, enc_hidden)\n",
        "                        \n",
        "                        \n",
        "                        loss = 0\n",
        "                      \n",
        "                        with tf.GradientTape() as tape:\n",
        "                              enc_output, enc_hidden = encoder(inp, enc_hidden,train = False)\n",
        "                              # print(enc_output[0][0][0])\n",
        "                              dec_hidden = enc_hidden\n",
        "                              \n",
        "                              dec_input = tf.expand_dims([y_tokenizer.word_index['<start>']] * Batch_Size, 1)\n",
        "                              for t in range(1, targ.shape[1]):\n",
        "                                  # passing enc_output to the decoder\n",
        "                                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "                                  loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "                                  # using teacher forcing\n",
        "                                  dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "                              \n",
        "                        \n",
        "                        batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "                        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "                        gradients = tape.gradient(loss, variables)\n",
        "\n",
        "                        optimizer.apply_gradients(zip(gradients, variables))\n",
        "                        total_loss += batch_loss\n",
        "                        \n",
        "                        if verbose:\n",
        "                            \n",
        "                            if batch % verbose == 0:\n",
        "                                  \n",
        "                                  print('Batch {} Loss {:.4f}'.format(batch,batch_loss.numpy()))\n",
        "\n",
        "                tf.summary.scalar('loss',total_loss/steps_per_epoch,step=epoch)\n",
        "                checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\".format(str(time.localtime().tm_hour)+'_'+str(time.localtime().tm_min)+'_'+str(time.localtime().tm_sec)))\n",
        "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "                \n",
        "                if (epoch + 1 ) % 5 == 0:\n",
        "                    print('\\n')\n",
        "                    print('Calculating score for train and test data it may take some time...')\n",
        "                    train_score = score_calc(type_ = 'bleu',x = x_train,y = y_train_)\n",
        "                    val_score   = score_calc(type_ = 'bleu',x = x_val,y = y_val_)\n",
        "                    tf.summary.scalar('Train_score',train_score,step=epoch)\n",
        "                    tf.summary.scalar('Test_score',val_score,step=epoch)\n",
        "                    print('----'*15)\n",
        "                    print('Epoch {} Loss {:.4f} Train Score {} Test Score {} Time {} sec'.format(epoch + 1, total_loss / steps_per_epoch,train_score,val_score,time.time() - start))\n",
        "                    print('----'*15)\n",
        "                \n",
        "                else:\n",
        "                  print('----'*15)\n",
        "                  print('Epoch {} Loss {:.4f} Time {} sec'.format(epoch + 1, total_loss / steps_per_epoch, time.time() - start))\n",
        "                  print('----'*15)\n",
        "\n",
        "                writer_flush = train_writer.flush()\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdbonKYiBuCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs/train_data/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rh1-vUqnnYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "train(EPOCHS=10,verbose = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "# function ConnectButton(){\n",
        "#     console.log(\"Connect pushed\"); \n",
        "#     document.querySelector(\"#connect\").click() \n",
        "# }\n",
        "# setInterval(ConnectButton,60000);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "colab": {}
      },
      "source": [
        "# # restoring the latest checkpoint in checkpoint_dir\n",
        "# checkpoint_dir = 'check_points'\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP0u4yp4i5w4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# # Moving checkpoints to drive\n",
        "# for i in os.listdir('checkpoints'):\n",
        "#   shutil.move('checkpoints/'+i,'gdrive/My Drive/google/check_points/with_to_missplaced/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VosIN4ePvz49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WrAM0FDomq3E",
        "colab": {}
      },
      "source": [
        "# # Correct     : 5,6,12,15,16,18,20,27,35,37,40,41\n",
        "# # Little Wrong: 4,8,10,14,29\n",
        "# # Interesting : 32,38\n",
        "# id_ = 1\n",
        "# print(x_val_[id_])\n",
        "# # print(x_val_[id_][:-9])\n",
        "# print('Actual y:',y_val_[id_])\n",
        "# result, pred,pred_prob = predict_next_word(x_val_[id_])\n",
        "# print('Pred   y:',result)\n",
        "\n",
        "\n",
        "# Log = np.log(pred_prob)\n",
        "# print(np.dot(np.nan_to_num(Log),pred_prob))\n",
        "# print(\"Log\",Log)\n",
        "# print(\"Prob\",pred_prob)\n",
        "# final_perp = []\n",
        "# for i in [5,6,12,15,16,18,20,27,35,37,40,41]:\n",
        "#   result,pred,pred_prob = predict_next_word(x_val_[i])\n",
        "#   Log = np.log(pred_prob)\n",
        "#   print('Actual y :',y_val_[i])\n",
        "#   print('Pred   y :',result)\n",
        "#   print('Pred_prob:',[j.numpy() for j in pred_prob])\n",
        "#   print('Log prob :',Log)\n",
        "#   print(np.dot(Log,pred_prob))\n",
        "#   final_perp.append(np.dot(Log,pred_prob))\n",
        "#   print('='*10)\n",
        "\n",
        "# -sum(final_perp)/len(final_perp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC4iUQTBhabQ",
        "colab_type": "text"
      },
      "source": [
        "## We shall check the  model with some unseen data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJGU2li6s5Vw",
        "colab_type": "text"
      },
      "source": [
        "<strong> Most of the predictions are correct, every time model id predicting 3 or more words</strong> <br>NOTE: In actual section i have placed all next comming words(to make understad for readers) model only predicts minimum one word and maximum five words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Ddd3JsL1tN",
        "colab_type": "text"
      },
      "source": [
        "<strong>Pytorch takes less time when compared to Tensorflow for  Google NMT models:</strong>https://medium.com/syncedreview/tensorflow-pytorch-or-mxnet-a-comprehensive-evaluation-on-nlp-cv-tasks-with-titan-rtx-cdf816fc3935"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZC543DJskfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gltkTugrmxnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSx2iM36EZQZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}